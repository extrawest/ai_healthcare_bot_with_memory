from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from mem0 import Memory

from src.config.settings import settings
from src.utils import setup_logger

logger = setup_logger(__name__)


class AIHealthcareSupport:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(AIHealthcareSupport, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        """
        Initialize the AI Healthcare Support with Memory Configuration and Langchain OpenAI Chat Model.
        """
        if not hasattr(self, '_initialized') or not self._initialized:
            self._initialized = True

        custom_prompt = """
        Please only extract entities containing patient health information, appointment details, and user information. 
        Here are some few shot examples:

        Input: Hi.
        Output: {{"facts" : []}}

        Input: The weather is nice today.
        Output: {{"facts" : []}}

        Input: I have a headache and would like to schedule an appointment.
        Output: {{"facts" : ["Patient reports headache", "Wants to schedule an appointment"]}}

        Input: My name is Jane Smith, and I need to reschedule my appointment for next Tuesday.
        Output: {{"facts" : ["Patient name: Jane Smith", "Wants to reschedule appointment", "Original appointment: next Tuesday"]}}

        Input: I have diabetes and my blood sugar is high.
        Output: {{"facts" : ["Patient has diabetes", "Reports high blood sugar"]}}

        Return the facts and patient information in a json format as shown above.
        """

        config = {
            "llm": {
                "provider": "openai",
                "config": {
                    "model": "gpt-4o",
                    "temperature": 0.1,
                    "max_tokens": 2000,
                    "api_key": settings.openai_api_key
                }
            },
            "embedder": {
                "provider": "openai",
                "config": {
                    "model": "text-embedding-3-large",
                    "api_key": settings.openai_api_key
                }
            },
            "vector_store": {
                "provider": "qdrant",
                "config": {
                    "collection_name": "ai_healthcare_support",
                    "embedding_model_dims": 1536,
                    "host": settings.qdrant_host,
                    "port": settings.qdrant_port
                }
            },
            "custom_prompt": custom_prompt,
            "version": "v1.1",
        }

        self.memory = Memory.from_config(config)
        self.app_id = "app-1"
        self.model = ChatOpenAI(model="gpt-4o", api_key=settings.openai_api_key)

    def ask(self, question, user_id=None):
        """
        Ask a question the AI and store the relevant facts in memory.

        :param question: The question to ask the AI.
        :param user_id: Optional user ID to associate with the memory.
        :return: Response from the AI along with the original question.
        """
        logger.info("Self ID: {}".format(id(self)))
        memories = self.search_memory(question, user_id=user_id)

        context = "Relevant information from previous conversations:\n"
        if memories['results']:
            for memory in memories['results']:
                context += f" - {memory['memory']}\n"

        messages = [
            SystemMessage(content=f"""You are a helpful healthcare support assistant. Use the provided context to personalize your responses and remember user health information and past interactions. {context}"""),
            HumanMessage(content=question)
        ]

        response = self.model.invoke(messages)

        self.add_memory(question, response.content, user_id=user_id)

        logger.info("Memories: {}".format(self.get_memories(user_id=user_id)))

        return {"messages": [response.content]}

    def add_memory(self, question, response, user_id=None):
        """
        Add a memory entry to the memory store.

        :param question: The question that was asked by the user.
        :param response: The response generated by the AI.
        :param user_id: Optional user ID to associate with the memory.
        """
        self.memory.add(f"User: {question}\nAssistant: {response}", user_id=user_id, metadata={"app_id": self.app_id})

    def get_memories(self, user_id=None):
        """
        Retrieve all memories associated with the given user ID.

        :param user_id: Optional user ID to filter memories.
        :return: List of memories.
        """
        return self.memory.get_all(user_id=user_id)

    def search_memory(self, query, user_id=None):
        """
        Search for memories related to the given query and user ID.

        :param query: The query to search for in the memories.
        :param user_id: Optional user ID to filter memories.
        :return: List of relevant memories.
        """
        related_memories = self.memory.search(query, user_id=user_id)
        return related_memories